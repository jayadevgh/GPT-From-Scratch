{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef8c0a95-61fb-4c65-b46b-a0bc4a30bc3f",
   "metadata": {},
   "source": [
    "Why should I understand backpropagation:\n",
    "\n",
    "- Vanishing non-linearities\n",
    "    As explored in the previous code, sigmoids can cause vanishing gradients as large initial weights can push inputs into saturated regions of the curve (the flat parts). Since the derivative is near zero, the neuron recieves near zero gradient, effectively freezing the neuron and potential learning.\n",
    "\n",
    "- Dying ReLUs\n",
    "- Exploding gradients in RNNs\n",
    "- Clipping values instead of using a proper loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ddf28-e9fe-4ff3-acdb-78d78b152764",
   "metadata": {},
   "source": [
    "What good would redoing the mlp code with manual backprop bring us:\n",
    "\n",
    "- good exercise\n",
    "- better at debugging netwroks\n",
    "- understand what you are doing\n",
    "- you won't be nervous about whats hidden away from u\n",
    "- emerge stronger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cf0212-e266-4a17-baa6-34b6fe0b55a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2caa2ed3-755f-44ad-a741-e14a14ddb826",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to read the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f66c3779-5e89-4ac6-8fbb-e4f49375c518",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(vocab_size)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f861d7-ff48-451e-85b8-b06433e46974",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "block_size = 3 # this represents how many letters of context the model considers before choosing the next letter\n",
    "\n",
    "def build_dataset(words):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "        #print(w)\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217d5942-4317-4a58-9e8d-128876d935d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#compare our gradients to pytorch gradients to see if we are accurate\n",
    "def cmp(s, dt, t):\n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea26888f-bf4a-483b-a2fc-7481609c9f43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "# from the previous code\n",
    "n_embd = 10 # dimensionality of character embedding vectors\n",
    "n_hidden = 64 # number of neurons in the hidden layers\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd),             generator=g) #embeddings\n",
    "#layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size) ** 0.5)# * 0.2 # best number to put this is divide by fanin (sqrt(n_embd * block_size))\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "#layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1 # sometimes when implemented as zero, it can mask the gradients\n",
    "\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "# bnmean_running = torch.zeros((1, n_hidden))\n",
    "# bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24985c1-09cb-4d08-8a58-23c9e66bf827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015aa8ff-e657-47ee-aaaf-56d042a507ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.3396, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xb] # embeds characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) #concatenate vectors\n",
    "# linear layer\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# Batch norm layer\n",
    "bnmeani = hprebn.sum(0, keepdim=True) * 1/n\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff ** 2\n",
    "bnvar = 1/(n-1) * bndiff2.sum(0, keepdim=True)\n",
    "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "#bnstdi = hprebn.std(0, keepdim=True)\n",
    "hpreact = bngain * bnraw + bnbias # batch normalization\n",
    "# Non-linearity activation\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear Layer 2\n",
    "logits = h @ W2 + b2 # output\n",
    "#cross entropy loss written out : loss = F.cross_entropy(logits, Ytr[ix]) # loss\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "#Pytorch back pass\n",
    "for p in parameters:\n",
    "    p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
    "        norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "        bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "        embcat, emb]:\n",
    "    t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb08ebb-73fb-4867-9c07-30118bf73276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 10]), torch.Size([27, 10]), torch.Size([32, 3]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.shape, C.shape, Xb.shape#, W1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d75e47a0-b82a-4440-bd1d-9a03834941dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnraw           | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bnvar_inv       | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "bnvar           | exact: False | approximate: True  | maxdiff: 6.984919309616089e-10\n",
      "bndiff2         | exact: False | approximate: True  | maxdiff: 2.1827872842550278e-11\n",
      "bndiff          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "hprebn          | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
      "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "W1              | exact: False | approximate: True  | maxdiff: 9.313225746154785e-09\n",
      "b1              | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
      "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
      "C               | exact: False | approximate: True  | maxdiff: 5.587935447692871e-09\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1: backpropagate through everything manually\n",
    "# backpropagating through every single variable \n",
    "# as they are defined in the forward pass one by one\n",
    "\n",
    "# YOUR CODE HERE :)\n",
    "# \n",
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = 1.0/probs * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts_sum = -counts_sum**-2 * dcounts_sum_inv\n",
    "dcounts = counts_sum_inv * dprobs + dcounts_sum * torch.ones_like(counts)\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogit_maxes = -dnorm_logits.sum(1, keepdim=True)\n",
    "dlogits = dnorm_logits + F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T \n",
    "dW2 = h.T @ dlogits\n",
    "db2 = dlogits.sum(0)\n",
    "dhpreact = (1.0 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = (-0.5 * (bnvar + 1e-5) ** -1.5) * dbnvar_inv\n",
    "dbndiff2 = 1/(n-1) * dbnvar * torch.ones_like(bndiff2)\n",
    "dbndiff = bnvar_inv * dbnraw + (2 * bndiff * dbndiff2)\n",
    "dbnmeani = -(dbndiff.sum(0, keepdim=True))\n",
    "dhprebn = dbndiff + (1/n * dbnmeani * torch.ones_like(hprebn))\n",
    "dembcat = dhprebn @ W1.T \n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "    for j in range(Xb.shape[1]):\n",
    "        ix = Xb[k,j]\n",
    "        dC[ix] += demb[k,j]\n",
    "\n",
    "cmp('logprobs', dlogprobs, logprobs)\n",
    "cmp('probs', dprobs, probs)\n",
    "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "cmp('counts_sum', dcounts_sum, counts_sum)\n",
    "cmp('counts', dcounts, counts)\n",
    "cmp('norm_logits', dnorm_logits, norm_logits)\n",
    "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "cmp('logits', dlogits, logits)\n",
    "cmp('h', dh, h)\n",
    "cmp('W2', dW2, W2)\n",
    "cmp('b2', db2, b2)\n",
    "cmp('hpreact', dhpreact, hpreact)\n",
    "cmp('bngain', dbngain, bngain)\n",
    "cmp('bnbias', dbnbias, bnbias)\n",
    "cmp('bnraw', dbnraw, bnraw)\n",
    "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "cmp('bnvar', dbnvar, bnvar)\n",
    "cmp('bndiff2', dbndiff2, bndiff2)\n",
    "cmp('bndiff', dbndiff, bndiff)\n",
    "cmp('bnmeani', dbnmeani, bnmeani)\n",
    "cmp('hprebn', dhprebn, hprebn)\n",
    "cmp('embcat', dembcat, embcat)\n",
    "cmp('W1', dW1, W1)\n",
    "cmp('b1', db1, b1)\n",
    "cmp('emb', demb, emb)\n",
    "cmp('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abfb4a0b-8159-4816-9bbe-1c95062f4e23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3396246433258057 diff: 2.384185791015625e-07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Exercise 2: backprop through cross_entropy but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the loss,\n",
    "# take the derivative, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# logit_maxes = logits.max(1, keepdim=True).values\n",
    "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "# counts = norm_logits.exp()\n",
    "# counts_sum = counts.sum(1, keepdims=True)\n",
    "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "# probs = counts * counts_sum_inv\n",
    "# logprobs = probs.log()\n",
    "# loss = -logprobs[range(n), Yb].mean()\n",
    "\n",
    "# now:\n",
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c522958-43c7-4853-87b3-bd4e5dfafd60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 6.28642737865448e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# YOUR CODE HERE :)\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b8867-76eb-4d28-af15-6993d63e410b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Gradient of Cross-Entropy Loss with Softmax**\n",
    "\n",
    "We consider the cross-entropy loss applied to the output of a softmax:\n",
    "\n",
    "$$\n",
    "L(z, y) = -\\log\\left( \\frac{e^{z_y}}{\\sum_{k=1}^C e^{z_k}} \\right)\n",
    "$$\n",
    "\n",
    "Let $ z \\in \\mathbb{R}^C $ be the logits vector for a single sample with $ C $ classes, and let $ y \\in \\{0, \\dots, C-1\\} $ be the index of the correct class.\n",
    "\n",
    "Define the softmax:\n",
    "\n",
    "$$\n",
    "p_j = \\text{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^C e^{z_k}} = \\frac{e^{z_j}}{S}, \\quad \\text{where } S = \\sum_{k=1}^C e^{z_k}\n",
    "$$\n",
    "\n",
    "Then the cross-entropy loss becomes:\n",
    "\n",
    "$$\n",
    "L(z, y) = -\\log(p_y) = -\\log\\left( \\frac{e^{z_y}}{S} \\right) = -z_y + \\log\\left( \\sum_{k=1}^C e^{z_k} \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient with Respect to Logits $ z_j $**\n",
    "\n",
    "We compute:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = \\frac{\\partial}{\\partial z_j} \\left( \\log\\left( \\sum_{k=1}^C e^{z_k} \\right) - z_y \\right)\n",
    "$$\n",
    "\n",
    "This separates into two parts:\n",
    "\n",
    "#### 1. Derivative of the log-sum-exp term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j} \\log\\left( \\sum_{k=1}^C e^{z_k} \\right)\n",
    "= \\frac{1}{\\sum_{k=1}^C e^{z_k}} \\cdot \\frac{\\partial}{\\partial z_j} \\left( \\sum_{k=1}^C e^{z_k} \\right)\n",
    "= \\frac{e^{z_j}}{\\sum_{k=1}^C e^{z_k}} = p_j\n",
    "$$\n",
    "\n",
    "#### 2. Derivative of the correct class term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial z_j} (-z_y) =\n",
    "\\begin{cases}\n",
    "-1 & \\text{if } j = y \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "= -\\delta_{jy}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Gradient (Per Sample)**\n",
    "\n",
    "Combining both terms:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial z_j} = p_j - \\delta_{jy}\n",
    "$$\n",
    "\n",
    "In vector form:\n",
    "\n",
    "$$\n",
    "\\nabla_z L = \\text{softmax}(z) - \\text{one\\_hot}(y)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Batch Case**\n",
    "\n",
    "For a batch of $ n $ samples:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{n} \\sum_{i=1}^n L(z^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "The corresponding gradient is:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n} \\sum_{i=1}^n \\left( \\text{softmax}(z^{(i)}) - \\text{one\\_hot}(y^{(i)}) \\right)\n",
    "$$\n",
    "\n",
    "Which can be implemented in PyTorch as:\n",
    "\n",
    "```python\n",
    "dlogits = F.softmax(logits, dim=1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916307c7-7bb6-4849-b340-0144e8262f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42eeef53-b532-4a74-a58d-e0b548be2b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0710, 0.0880, 0.0186, 0.0500, 0.0196, 0.0826, 0.0242, 0.0358, 0.0180,\n",
       "        0.0315, 0.0370, 0.0364, 0.0371, 0.0287, 0.0349, 0.0137, 0.0091, 0.0194,\n",
       "        0.0158, 0.0541, 0.0500, 0.0215, 0.0253, 0.0711, 0.0590, 0.0260, 0.0215],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad9dd07d-57ec-4a29-970f-869c6bec46f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0710,  0.0880,  0.0186,  0.0500,  0.0196,  0.0826,  0.0242,  0.0358,\n",
       "        -0.9820,  0.0315,  0.0370,  0.0364,  0.0371,  0.0287,  0.0349,  0.0137,\n",
       "         0.0091,  0.0194,  0.0158,  0.0541,  0.0500,  0.0215,  0.0253,  0.0711,\n",
       "         0.0590,  0.0260,  0.0215], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31efe2a1-acad-4f24-a909-d11f5e31a2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4925e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c4e1e56-8cd6-455a-97bc-484f7ce48000",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f70e41142d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74e2f9be-f0b0-4f84-b3d7-a9746729fe8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3: backprop through batchnorm but all in one go\n",
    "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
    "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
    "\n",
    "# forward pass\n",
    "\n",
    "# before:\n",
    "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "# bndiff = hprebn - bnmeani\n",
    "# bndiff2 = bndiff**2\n",
    "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "# bnraw = bndiff * bnvar_inv\n",
    "# hpreact = bngain * bnraw + bnbias\n",
    "\n",
    "# now:\n",
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c88b9ac6-c319-4dc8-bb74-17342017450e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "# before we had:\n",
    "# dbnraw = bngain * dhpreact\n",
    "# dbndiff = bnvar_inv * dbnraw\n",
    "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
    "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
    "# dbndiff += (2*bndiff) * dbndiff2\n",
    "# dhprebn = dbndiff.clone()\n",
    "# dbnmeani = (-dbndiff).sum(0)\n",
    "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "\n",
    "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
    "# (you'll also need to use some of the variables from the forward pass up above)\n",
    "\n",
    "# YOUR CODE HERE :)\n",
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7d565-7359-4454-9ec3-fab397c06e4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Backpropagation through Batch Normalization: Step-by-Step Derivation\n",
    "\n",
    "This is a derivation of the **backpropagation for batch normalization**, specifically calculating gradients of the loss with respect to the input $x_i$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Big Picture: Whatâ€™s Going On\n",
    "\n",
    "Batch norm:\n",
    "\n",
    "1. Normalize input $x_i$ using batch mean $\\mu$ and variance $\\sigma^2$.\n",
    "2. Then scale and shift using parameters $\\gamma, \\beta$.\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}, \\quad y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "Goal: compute\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 0: Definitions\n",
    "\n",
    "- $\\mu = \\frac{1}{m} \\sum_i x_i$ (batch mean)  \n",
    "- $\\sigma^2 = \\frac{1}{m-1} \\sum_i (x_i - \\mu)^2$ (batch variance)  \n",
    "- $\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$ (normalized)  \n",
    "- $y_i = \\gamma \\hat{x}_i + \\beta$ (scaled and shifted output)  \n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Chain Rule for $\\frac{\\partial L}{\\partial x_i}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}_i} \\frac{\\partial \\hat{x}_i}{\\partial x_i} + \\frac{\\partial L}{\\partial \\mu} \\frac{\\partial \\mu}{\\partial x_i} + \\frac{\\partial L}{\\partial \\sigma^2} \\frac{\\partial \\sigma^2}{\\partial x_i}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Derivative w.r.t. $\\hat{x}_i$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\hat{x}_i} = \\frac{\\partial L}{\\partial y_i} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Derivative w.r.t. Variance $\\sigma^2$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\sigma^2} = \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} \\cdot \\gamma \\cdot (x_j - \\mu) \\cdot \\left(-\\frac{1}{2}\\right) (\\sigma^2 + \\epsilon)^{-3/2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Derivative w.r.t. Mean $\\mu$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\mu} = \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} \\cdot \\gamma \\cdot (-1) (\\sigma^2 + \\epsilon)^{-1/2} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{-2}{m-1} \\sum_i (x_i - \\mu)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: Combine all to get $\\frac{\\partial L}{\\partial x_i}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\frac{\\partial L}{\\partial \\hat{x}_i} \\cdot \\frac{1}{\\sqrt{\\sigma^2 + \\epsilon}} + \\frac{\\partial L}{\\partial \\sigma^2} \\cdot \\frac{2}{m-1}(x_i - \\mu) + \\frac{\\partial L}{\\partial \\mu} \\cdot \\frac{1}{m}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### Final Simplified Formula\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = \\gamma (\\sigma^2 + \\epsilon)^{-\\frac{1}{2}} \\left[ \\frac{\\partial L}{\\partial \\hat{x}_i} - \\frac{1}{m} \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} - \\frac{x_i - \\mu}{\\sigma^2 + \\epsilon} \\cdot \\frac{1}{m} \\sum_j \\frac{\\partial L}{\\partial \\hat{x}_j} (x_j - \\mu) \\right]\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be89f37b-7b0c-4706-b1ae-1f5976a8afe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12297\n",
      "      0/ 200000: 3.7931\n",
      "  10000/ 200000: 2.1890\n",
      "  20000/ 200000: 2.4020\n",
      "  30000/ 200000: 2.4839\n",
      "  40000/ 200000: 1.9834\n",
      "  50000/ 200000: 2.4230\n",
      "  60000/ 200000: 2.4006\n",
      "  70000/ 200000: 2.0099\n",
      "  80000/ 200000: 2.3982\n",
      "  90000/ 200000: 2.1853\n",
      " 100000/ 200000: 1.9021\n",
      " 110000/ 200000: 2.3510\n",
      " 120000/ 200000: 1.9526\n",
      " 130000/ 200000: 2.4242\n",
      " 140000/ 200000: 2.2631\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m hprebn \u001b[38;5;241m=\u001b[39m embcat \u001b[38;5;241m@\u001b[39m W1 \u001b[38;5;241m+\u001b[39m b1 \u001b[38;5;66;03m# hidden layer pre-activation\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# BatchNorm layer\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# -------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m bnmean \u001b[38;5;241m=\u001b[39m \u001b[43mhprebn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m bnvar \u001b[38;5;241m=\u001b[39m hprebn\u001b[38;5;241m.\u001b[39mvar(\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     50\u001b[0m bnvar_inv \u001b[38;5;241m=\u001b[39m (bnvar \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Exercise 4: putting it all together!\n",
    "# Train the MLP neural net with your own backward pass\n",
    "\n",
    "# init\n",
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# same optimization as last time\n",
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "n = batch_size # convenience\n",
    "lossi = []\n",
    "\n",
    "# use this context manager for efficiency once your backward pass is written (TODO)\n",
    "with torch.no_grad():\n",
    "\n",
    "  # kick off optimization\n",
    "  for i in range(max_steps):\n",
    "\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    # Linear layer\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    # BatchNorm layer\n",
    "    # -------------------------------------------------------------\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    # -------------------------------------------------------------\n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb) # loss function\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
    "\n",
    "    # manual backprop! #swole_doge_meme\n",
    "    # -----------------\n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # 2nd layer backprop\n",
    "    dh = dlogits @ W2.T\n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    # tanh\n",
    "    dhpreact = (1.0 - h**2) * dh\n",
    "    # batchnorm backprop\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # 1st layer\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    db1 = dhprebn.sum(0)\n",
    "    # embedding\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k,j]\n",
    "            dC[ix] += demb[k,j]\n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    # -----------------\n",
    "\n",
    "    # update\n",
    "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "    for p, grad in zip(parameters, grads):\n",
    "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
    "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
    "\n",
    "    # track stats\n",
    "    if i % 10000 == 0: # print every once in a while\n",
    "        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "    lossi.append(loss.log10().item())\n",
    "\n",
    "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
    "  #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e02af72-489d-4a1c-98bd-f92e6ebc7fcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # pass the training set through\n",
    "    emb = C[Xtr]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    # measure the mean/std over the entire training set\n",
    "    bnmean = hpreact.mean(0, keepdim=True)\n",
    "    bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d7db82c3-3379-40fc-a2d0-cf5d043c4e76",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 2.0732405185699463\n",
      "val 2.1073310375213623\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad() # this decorator disables gradient tracking\n",
    "def split_loss(split):\n",
    "    x,y = {\n",
    "        'train': (Xtr, Ytr),\n",
    "        'val': (Xdev, Ydev),\n",
    "        'test': (Xte, Yte),\n",
    "    }[split]\n",
    "    emb = C[x] # (N, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "    hpreact = embcat @ W1 + b1\n",
    "    hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "    logits = h @ W2 + b2 # (N, vocab_size)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(split, loss.item())\n",
    "\n",
    "split_loss('train')\n",
    "split_loss('val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08ace5a3-0a0c-4110-ae83-14a48de95d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mona.\n",
      "kayah.\n",
      "seel.\n",
      "ndhayla.\n",
      "reisha.\n",
      "ejdrie.\n",
      "cadered.\n",
      "elin.\n",
      "shi.\n",
      "jen.\n",
      "eden.\n",
      "estanaraelyzion.\n",
      "kalin.\n",
      "shubergshiriel.\n",
      "kindreelyn.\n",
      "jose.\n",
      "casube.\n",
      "geder.\n",
      "yarue.\n",
      "els.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "        # ------------\n",
    "        # forward pass:\n",
    "        # Embedding\n",
    "        emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "        embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "        hpreact = embcat @ W1 + b1\n",
    "        hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "        h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "        logits = h @ W2 + b2 # (N, vocab_size)\n",
    "        # ------------\n",
    "        # Sample\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        context = context[1:] + [ix]\n",
    "        out.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83286fe5-35ee-4db0-8d47-57d35523d3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
